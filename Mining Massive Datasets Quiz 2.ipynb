{
 "metadata": {
  "name": "",
  "signature": "sha256:3490e2980ccb74625a7be9ee26c8038479b21f3a69ca6ff8c1bf7051859fee7a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Basic"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Question 2 (minhashing):"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Consider the following matrix. Find minhash values for the following ordering of the rows (R4, R6, R1, R3, R5, R2). \n",
      "\n",
      "__Recall:__ The minhash value of a column is index of the first 1 entry under a permutation of the rows."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = {'C1': [0, 1, 0, 0, 1, 0],\n",
      "        'C2': [1, 0, 1, 0, 0, 1],\n",
      "        'C3': [1, 1, 0, 1, 1, 0],\n",
      "        'C4': [0, 1, 1, 0, 0, 0]}\n",
      "\n",
      "df = pd.DataFrame(data)\n",
      "print df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "   C1  C2  C3  C4\n",
        "0   0   1   1   0\n",
        "1   1   0   1   1\n",
        "2   0   1   0   1\n",
        "3   0   0   1   0\n",
        "4   1   0   1   0\n",
        "5   0   1   0   0\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "permute = {0:4, 1:6, 2:1, 3:3, 4:5, 5:2}\n",
      "\n",
      "for _ in range(1,5):\n",
      "    s = df[\"C{}\".format(_)]\n",
      "    new = [s[3], s[5], s[0],s[2], s[4], s[1]]\n",
      "    idx = new.index(1)\n",
      "    print \"MinHash of column {0} is row {1}\".format(_, permute[idx])\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "MinHash of column 1 is row 5\n",
        "MinHash of column 2 is row 6\n",
        "MinHash of column 3 is row 4\n",
        "MinHash of column 4 is row 3\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Question 3 (signatures):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = {'C1': [1,2,3,4,5,6],\n",
      "        'C2': [2,3,1,1,2,1],\n",
      "        'C3': [1,4,2,3,5,6],\n",
      "        'C4': [1,2,3,1,1,4],\n",
      "        'C5': [2,3,1,2,1,1],\n",
      "        'C6': [5,2,3,4,5,1],\n",
      "        'C7': [4,2,2,4,1,4]}\n",
      "\n",
      "df = pd.DataFrame(data)\n",
      "print df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "   C1  C2  C3  C4  C5  C6  C7\n",
        "0   1   2   1   1   2   5   4\n",
        "1   2   3   4   2   3   2   2\n",
        "2   3   1   2   3   1   3   2\n",
        "3   4   1   3   1   2   4   4\n",
        "4   5   2   5   1   1   5   1\n",
        "5   6   1   6   4   1   1   4\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Question 4 (shingles):"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Find the set of 2-shingles for the \"document\" ABRACADABRA and also for the \"document\" BRICABRAC.\n",
      "\n",
      "Answer the following questions:\n",
      "\n",
      "How many 2-shingles does ABRACADABRA have?\n",
      "How many 2-shingles does BRICABRAC have?\n",
      "How many 2-shingles do they have in common?\n",
      "What is the Jaccard similarity between the two documents\"?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def getKshingles(string,k):\n",
      "    shingles = set()\n",
      "    for idx in xrange(k,len(string)):\n",
      "        shingles.add(string[idx - k:idx])\n",
      "    return shingles\n",
      "\n",
      "abra = getKshingles(\"ABRACADABRA\", 2)\n",
      "print abra\n",
      "bric = getKshingles(\"BRICABRAC\", 2)\n",
      "print bric\n",
      "\n",
      "inter = abra.intersection(bric)\n",
      "print inter\n",
      "\n",
      "union = abra.union(bric)\n",
      "print union\n",
      "\n",
      "print \"Jaccard similiaryt is {}\".format(len(inter)*1.0/len(union))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "set(['AC', 'AB', 'AD', 'CA', 'DA', 'RA', 'BR'])\n",
        "set(['AB', 'CA', 'RA', 'BR', 'IC', 'RI'])\n",
        "set(['CA', 'AB', 'RA', 'BR'])\n",
        "set(['AC', 'AB', 'AD', 'CA', 'DA', 'RA', 'BR', 'IC', 'RI'])\n",
        "Jaccard similiaryt is 0.444444444444\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Question 6 (5 is omitted):"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For the following points, find whether they are closer to (0,0) or (100,40) under the L1 and L2 norms:\n",
      "\n",
      "(53,15)\n",
      "(63,8)\n",
      "(53,18)\n",
      "(54,8)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def distance(pt1, pt2, norm=2):\n",
      "    differences = sum(abs(coord1-coord2)**norm for coord1, coord2 in zip(pt1,pt2))\n",
      "    return np.power(differences, 1./norm)\n",
      "\n",
      "for pt1 in ((53,15), (63,8), (53,18), (54,8)):\n",
      "    print pt1\n",
      "    for norm in range(1,3):\n",
      "        pt2 = ((0,0), (100,40))\n",
      "        one = distance(pt1, pt2[0], norm=norm)\n",
      "        two = distance(pt1, pt2[1], norm=norm)\n",
      "        if one < two:\n",
      "            print pt1, \"assigned to\", pt2[0], \"under L{} norm\".format(norm)\n",
      "        else:\n",
      "            print pt1, \"assigned to\", pt2[1], \"under L{} norm\".format(norm)\n",
      "    print '*' * 10"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(53, 15)\n",
        "(53, 15) assigned to (0, 0) under L1 norm\n",
        "(53, 15) assigned to (100, 40) under L2 norm\n",
        "**********\n",
        "(63, 8)\n",
        "(63, 8) assigned to (100, 40) under L1 norm\n",
        "(63, 8) assigned to (100, 40) under L2 norm\n",
        "**********\n",
        "(53, 18)\n",
        "(53, 18) assigned to (100, 40) under L1 norm\n",
        "(53, 18) assigned to (100, 40) under L2 norm\n",
        "**********\n",
        "(54, 8)\n",
        "(54, 8) assigned to (0, 0) under L1 norm\n",
        "(54, 8) assigned to (0, 0) under L2 norm\n",
        "**********\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Advanced"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "__Question 1__\n",
      "\n",
      "Memory (S) needed to run the A-priori algorithm where 1 million total items, N denotes the number of frequent items (occur more than s=10000 times). 1 million pairs occur more than 10000 times. 2M pairs occur once, M of these have two frequent items, M contain at least one non-frequent item - no other pairs occur.\n",
      "\n",
      "Recall the idea of the A-priori algorithm - each item in a frequent item pair (occurs more than s times) must itself occur more than s times. This idea can be extended when moving from frequent item k-sets to frequent itm (k+1)-sets"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "__Hint:__ When considering the hash table for counting pairs of frequent items that actually occur in the data set, remember that you need 12 bytes per entry, 4 each to store the two item ID's and 4 to store the integer count. The number of 12-byte entries will be the number of pairs that occur in the data and have both items frequent."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s = \"\"\"N = 50,000; M = 40,000,000; S = 800,000,000\n",
      "N = 100,000; M = 100,000,000; S = 1,200,000,000\n",
      "N = 20,000; M = 60,000,000; S = 1,000,000,000\n",
      "N = 50,000; M = 80,000,000; S = 1,500,000,000\"\"\"\n",
      "\n",
      "\n",
      "s = s.split(\"N\")\n",
      "for thing in [\"=\", \" \", \",\", \"S\", \"M\"]:\n",
      "    s = [re.sub(thing, \"\", t).rstrip(\"\\n\") for t in s if len(t)]\n",
      "\n",
      "triple = []\n",
      "\n",
      "for t in s:\n",
      "    triple.append(map(int, t.split(\";\")))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 60
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for _ in xrange(4):\n",
      "    N, M, S  = triple[_]\n",
      "    #assert N*(N-1)/2. > M\n",
      "    \n",
      "    memory = min(12*(M + 10**6), 4*N+2*N*(N-1))\n",
      "    \n",
      "    # Hash\n",
      "    # 1 million frequent pairs (we will automatically get these from only looking at frequent items\n",
      "    # there are 2 million pairs that appear only once, M of which have both items being frequent- the counts for these pairs will\n",
      "    # also be stored.\n",
      "    \n",
      "    # triangular array method -- 1./2*N*(N-1) for triangular array of frequent items, 4 bytes each plus the list of integers for\n",
      "    # the frequent items themselves\n",
      "    \n",
      "    \n",
      "    print S - memory#, memory, S"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "320000000\n",
        "0\n",
        "280000000\n",
        "540000000\n"
       ]
      }
     ],
     "prompt_number": 61
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "__Question 1(Advanced):__\n",
      "    \n",
      "Suppose we perform the PCY algorithm to find frequent pairs, with market-basket data meeting the following specifications:\n",
      "\n",
      "* s, the support threshold, is 10,000.\n",
      "* There are one million items, which are represented by the integers 0,1,...,999999.\n",
      "* There are 250,000 frequent items, that is, items that occur 10,000 times or more.\n",
      "* There are one million pairs that occur 10,000 times or more.\n",
      "* There are P pairs that occur exactly once and consist of 2 frequent items.\n",
      "* No other pairs occur at all.\n",
      "* Integers are always represented by 4 bytes.\n",
      "* When we hash pairs, they distribute among buckets randomly, but as evenly as possible; i.e., you may assume that each bucket gets exactly its fair share of the P pairs that occur once.\n",
      "\n",
      "Suppose there are S bytes of main memory. In order to run the PCY algorithm successfully, the number of buckets must be sufficiently large that most buckets are not large. In addition, on the second pass, there must be enough room to count all the candidate pairs. As a function of S, what is the largest value of P for which we can successfully run the PCY algorithm on this data? Demonstrate that you have the correct formula by indicating which of the following is a value for S and a value for P that is approximately (i.e., to within 10%) the largest possible value of P for that S."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "s = \"\"\"S = 300,000,000; P = 750,000,000\n",
      "S = 100,000,000; P = 540,000,000\n",
      "S = 500,000,000; P = 5,000,000,000\n",
      "S = 1,000,000,000; P = 35,000,000,000\"\"\"\n",
      "\n",
      "s = s.split(\"S\")\n",
      "for thing in [\"=\", \" \", \",\", \"P\"]:\n",
      "    s = [re.sub(thing, \"\", t).rstrip(\"\\n\") for t in s if len(t)]\n",
      "\n",
      "pair = []\n",
      "\n",
      "for t in s:\n",
      "    pair.append(map(int, t.split(\";\")))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for _ in xrange(4):\n",
      "    S, P  = pair[_]\n",
      "    \n",
      "    S= S/4.\n",
      "    print S - 3*(10**6)*(1+float(P)/S)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "42000000.0\n",
        "-42800000.0\n",
        "2000000.0\n",
        "-173000000.0\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "My approach for this problem is calculating the memory S needed to accommodate P, rather than figuring out the P that can fit into S. It answers the same question, just at a different angle.\n",
      "\n",
      "Another thing I would do is converting memory S from Bytes to Integers (i.e. S = S/4) to normalize it with other numbers given by the question. In other words, S is now the number of integers that can be stored in the memory.\n",
      "\n",
      "With that being said, the memory S must be big enough to store:\n",
      "The translate table that translates item names to integers\n",
      "The array of counts for single items (to find which ones are frequent)\n",
      "The data structure needed to handle pairs of items:\n",
      "For the first pass, it\u2019s the PCY buckets, each of which is an integer that keeps count of the number of occurrences of the pairs that are hashed into it.\n",
      "For the second pass, it\u2019s the bitmap that represents the PYC buckets from the first pass AND the triples that keep count of the candidate pairs (Note that we cannot use triangular matrix method for PCY)\n",
      "Since the items are already integers, we do not need to reserve space for (1). Also, the space needed for (2) scales with the number of items, which is negligible comparing to the amount of memory available, so we could ignore it as well for simplicity\u2019s sake. That leaves us with only (3).\n",
      "\n",
      "In the first pass, we have the entire memory S (in integers) to store the buckets B (also in integers). To reduce the number of triples to store for the second pass, we want B to be as large as possible (explained later). That means B = S.\n",
      "\n",
      "Because our hash function distributes P evenly among the buckets, each bucket will be the hash for P/B non-frequent pairs. Since each of these non-frequent pairs also occurs exactly ONCE, its contribution to the score of the bucket it is hashed to is ONE. That means P/B is also the score contributed by the non-frequent pairs in each bucket. You should find that P/B is generally less than 200, which is nowhere near the frequent threshold of 10,000 (Else PCY would be irrelevant).\n",
      "\n",
      "We can conclude that P does not affect whether a bucket is frequent or not, and it is entirely up to the frequent pairs M. So, for a bucket to meet the threshold, it must be the hash of at least one frequent pair. In the worst case scenario, there is no collision among M (i.e. no two elements of M are hashed into the same bucket), meaning there are a maximum of M frequent buckets. (You may wonder why this is the worst case. Note that in the second pass we will have to check all pairs in M and some pairs in P. While M is fixed, the more buckets we have to check, the more pairs in P we will have to check. So, a max number of buckets results in the most pairs in P get \u201cpiggy-backed\u201d into the second pass)\n",
      "\n",
      "In the second pass, the buckets are transformed into a bitmap which requires 1/32 of the memory needed to store the buckets (which was S itself). So we put aside S/32 for the bitmap, and the rest of the remaining memory T = (31/32)S is used to store the triples. (Many people answered before me argued that the memory needed for the bitmap is also negligible, and therefore T = S. For me, 1/32 of the entire memory is quite a significant number. It might not affect your answer since we have that 10% margin of error, but I believe it matters in real applications.)\n",
      "\n",
      "Now the question is, how many triples X do we have to store?\n",
      "\n",
      "In PCY, we have to keep count of pairs that meet the following requirement:\n",
      "Hashed into a frequent bucket.\n",
      "Made up of items that are frequent (inherited from A-Priori).\n",
      "Occurs at least once (due to Triple method)\n",
      "It is obvious that all pairs in M and in P satisfy (2) and (3) and we don\u2019t have to worry about pairs not in M nor P. We found out earlier that there are M frequent buckets. Each of these buckets is the hash for 1 frequent pair (to make it frequent) and P/B non-frequent pairs (due to even distribution of P). That means each frequent bucket represents 1+P/B pairs. In total, we have to have enough space to account for X = M(1+P/B) pairs. Since each of these pairs is represented by a triple of 3 integers in the memory, our final answer is:\n",
      "\n",
      "Total Memory Needed for the Triples = 3X = 3M(1+P/S) = T = (31/32)S\n",
      "\n",
      "From here you can plug in S and solve for P (or vice versa) and find answers within the 10% error (remember S here is in integers, not bytes like given in the question so you have to convert the S in the question into integers first). The answer I found only had 1.12% error"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}